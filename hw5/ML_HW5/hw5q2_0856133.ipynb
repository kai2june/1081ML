{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from libsvm.svmutil import *\n",
    "from libsvm.svm import *\n",
    "\n",
    "Y_train = open('Y_train.csv', 'r')\n",
    "X_train = open('X_train.csv', 'r')\n",
    "Y_test = open('Y_test.csv', 'r')\n",
    "X_test = open('X_test.csv', 'r')\n",
    "\n",
    "y = []\n",
    "for line in Y_train.readlines():\n",
    "    line = int(line)\n",
    "    y.append(line)\n",
    "print(y)\n",
    "\n",
    "x = []\n",
    "for line in X_train.readlines():\n",
    "    tmp = line.split(',')\n",
    "    elem = dict()\n",
    "    i = 1\n",
    "    for pixel in tmp:\n",
    "        pixel = float(pixel)\n",
    "        elem.update({i: pixel})\n",
    "        i += 1\n",
    "    x.append(elem)\n",
    "print(x)\n",
    "\n",
    "yt = []\n",
    "for line in Y_test.readlines():\n",
    "    line = int(line)\n",
    "    yt.append(line)\n",
    "print(yt)\n",
    "\n",
    "xt = []\n",
    "for line in X_test.readlines():\n",
    "    tmp = line.split(',')\n",
    "    elem = dict()\n",
    "    i = 1\n",
    "    for pixel in tmp:\n",
    "        pixel = float(pixel)\n",
    "        elem.update({i: pixel})\n",
    "        i += 1\n",
    "    xt.append(elem)\n",
    "print(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 95.08% (2377/2500) (classification)\n"
     ]
    }
   ],
   "source": [
    "# linear kernel\n",
    "#prob = svm_problem(y, x)\n",
    "#param = svm_parameter('-t 0 -b 1')\n",
    "model = svm_train(y, x, '-t 0 -b 1')\n",
    "p_label, p_acc, p_val = svm_predict(yt, xt, model)\n",
    "#print(p_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 95.76% (2394/2500) (classification)\n"
     ]
    }
   ],
   "source": [
    "# polynomial kernel\n",
    "model = svm_train(y, x, '-t 1 -b 1 -r 1')\n",
    "p_label, p_acc, p_val = svm_predict(yt, xt, model)\n",
    "#print(p_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 95.32% (2383/2500) (classification)\n"
     ]
    }
   ],
   "source": [
    "# RBF kernel\n",
    "model = svm_train(y, x, '-t 2 -b 1')\n",
    "p_label, p_acc, p_val = svm_predict(yt, xt, model)\n",
    "#print(p_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy = 97.16%\n",
      "c = 0.03125, gamma = 3.0517578125e-05, accuracy = 97.16\n",
      "Cross Validation Accuracy = 97.16%\n",
      "c = 0.03125, gamma = 0.0001220703125, accuracy = 97.16\n",
      "Cross Validation Accuracy = 97.06%\n",
      "c = 0.03125, gamma = 0.00048828125, accuracy = 97.06\n",
      "Cross Validation Accuracy = 97.12%\n",
      "c = 0.03125, gamma = 0.001953125, accuracy = 97.11999999999999\n",
      "Cross Validation Accuracy = 96.84%\n",
      "c = 0.03125, gamma = 0.0078125, accuracy = 96.84\n",
      "Cross Validation Accuracy = 97.06%\n",
      "c = 0.03125, gamma = 0.03125, accuracy = 97.06\n",
      "Cross Validation Accuracy = 97%\n",
      "c = 0.03125, gamma = 0.125, accuracy = 97.0\n",
      "Cross Validation Accuracy = 97.06%\n",
      "c = 0.03125, gamma = 0.5, accuracy = 97.06\n",
      "Cross Validation Accuracy = 97.4%\n",
      "c = 0.03125, gamma = 2, accuracy = 97.39999999999999\n",
      "Cross Validation Accuracy = 97.2%\n",
      "c = 0.03125, gamma = 8, accuracy = 97.2\n",
      "Cross Validation Accuracy = 97.12%\n",
      "c = 0.125, gamma = 3.0517578125e-05, accuracy = 97.11999999999999\n",
      "Cross Validation Accuracy = 96.98%\n",
      "c = 0.125, gamma = 0.0001220703125, accuracy = 96.98\n",
      "Cross Validation Accuracy = 96.9%\n",
      "c = 0.125, gamma = 0.00048828125, accuracy = 96.89999999999999\n",
      "Cross Validation Accuracy = 97.22%\n",
      "c = 0.125, gamma = 0.001953125, accuracy = 97.22\n",
      "Cross Validation Accuracy = 96.84%\n",
      "c = 0.125, gamma = 0.0078125, accuracy = 96.84\n",
      "Cross Validation Accuracy = 97.12%\n",
      "c = 0.125, gamma = 0.03125, accuracy = 97.11999999999999\n",
      "Cross Validation Accuracy = 96.86%\n",
      "c = 0.125, gamma = 0.125, accuracy = 96.86\n",
      "Cross Validation Accuracy = 97.06%\n",
      "c = 0.125, gamma = 0.5, accuracy = 97.06\n",
      "Cross Validation Accuracy = 97.02%\n",
      "c = 0.125, gamma = 2, accuracy = 97.02\n",
      "Cross Validation Accuracy = 97.1%\n",
      "c = 0.125, gamma = 8, accuracy = 97.1\n",
      "Cross Validation Accuracy = 96.66%\n",
      "c = 0.5, gamma = 3.0517578125e-05, accuracy = 96.66\n",
      "Cross Validation Accuracy = 96.34%\n",
      "c = 0.5, gamma = 0.0001220703125, accuracy = 96.34\n",
      "Cross Validation Accuracy = 96.46%\n",
      "c = 0.5, gamma = 0.00048828125, accuracy = 96.46000000000001\n",
      "Cross Validation Accuracy = 96.74%\n",
      "c = 0.5, gamma = 0.001953125, accuracy = 96.74000000000001\n",
      "Cross Validation Accuracy = 96.46%\n",
      "c = 0.5, gamma = 0.0078125, accuracy = 96.46000000000001\n",
      "Cross Validation Accuracy = 96.66%\n",
      "c = 0.5, gamma = 0.03125, accuracy = 96.66\n",
      "Cross Validation Accuracy = 96.58%\n",
      "c = 0.5, gamma = 0.125, accuracy = 96.58\n",
      "Cross Validation Accuracy = 96.6%\n",
      "c = 0.5, gamma = 0.5, accuracy = 96.6\n",
      "Cross Validation Accuracy = 96.64%\n",
      "c = 0.5, gamma = 2, accuracy = 96.64\n",
      "Cross Validation Accuracy = 96.92%\n",
      "c = 0.5, gamma = 8, accuracy = 96.92\n",
      "Cross Validation Accuracy = 96.8%\n",
      "c = 2, gamma = 3.0517578125e-05, accuracy = 96.8\n",
      "Cross Validation Accuracy = 96.48%\n",
      "c = 2, gamma = 0.0001220703125, accuracy = 96.48\n",
      "Cross Validation Accuracy = 96.58%\n",
      "c = 2, gamma = 0.00048828125, accuracy = 96.58\n",
      "Cross Validation Accuracy = 96.6%\n",
      "c = 2, gamma = 0.001953125, accuracy = 96.6\n",
      "Cross Validation Accuracy = 96.5%\n",
      "c = 2, gamma = 0.0078125, accuracy = 96.5\n",
      "Cross Validation Accuracy = 96.56%\n",
      "c = 2, gamma = 0.03125, accuracy = 96.56\n",
      "Cross Validation Accuracy = 96.74%\n",
      "c = 2, gamma = 0.125, accuracy = 96.74000000000001\n",
      "Cross Validation Accuracy = 96.4%\n",
      "c = 2, gamma = 0.5, accuracy = 96.39999999999999\n",
      "Cross Validation Accuracy = 96.56%\n",
      "c = 2, gamma = 2, accuracy = 96.56\n",
      "Cross Validation Accuracy = 96.34%\n",
      "c = 2, gamma = 8, accuracy = 96.34\n",
      "Cross Validation Accuracy = 96.46%\n",
      "c = 8, gamma = 3.0517578125e-05, accuracy = 96.46000000000001\n",
      "Cross Validation Accuracy = 96.7%\n",
      "c = 8, gamma = 0.0001220703125, accuracy = 96.7\n",
      "Cross Validation Accuracy = 96.66%\n",
      "c = 8, gamma = 0.00048828125, accuracy = 96.66\n",
      "Cross Validation Accuracy = 96.36%\n",
      "c = 8, gamma = 0.001953125, accuracy = 96.36\n",
      "Cross Validation Accuracy = 96.44%\n",
      "c = 8, gamma = 0.0078125, accuracy = 96.44\n",
      "Cross Validation Accuracy = 96.52%\n",
      "c = 8, gamma = 0.03125, accuracy = 96.52\n",
      "Cross Validation Accuracy = 96.52%\n",
      "c = 8, gamma = 0.125, accuracy = 96.52\n",
      "Cross Validation Accuracy = 96.74%\n",
      "c = 8, gamma = 0.5, accuracy = 96.74000000000001\n",
      "Cross Validation Accuracy = 96.52%\n",
      "c = 8, gamma = 2, accuracy = 96.52\n",
      "Cross Validation Accuracy = 96.74%\n",
      "c = 8, gamma = 8, accuracy = 96.74000000000001\n",
      "Cross Validation Accuracy = 96.52%\n",
      "c = 32, gamma = 3.0517578125e-05, accuracy = 96.52\n",
      "Cross Validation Accuracy = 96.42%\n",
      "c = 32, gamma = 0.0001220703125, accuracy = 96.41999999999999\n",
      "Cross Validation Accuracy = 96.46%\n",
      "c = 32, gamma = 0.00048828125, accuracy = 96.46000000000001\n",
      "Cross Validation Accuracy = 96.78%\n",
      "c = 32, gamma = 0.001953125, accuracy = 96.78\n",
      "Cross Validation Accuracy = 96.62%\n",
      "c = 32, gamma = 0.0078125, accuracy = 96.61999999999999\n",
      "Cross Validation Accuracy = 96.54%\n",
      "c = 32, gamma = 0.03125, accuracy = 96.54\n",
      "Cross Validation Accuracy = 96.64%\n",
      "c = 32, gamma = 0.125, accuracy = 96.64\n",
      "Cross Validation Accuracy = 96.4%\n",
      "c = 32, gamma = 0.5, accuracy = 96.39999999999999\n",
      "Cross Validation Accuracy = 96.56%\n",
      "c = 32, gamma = 2, accuracy = 96.56\n",
      "Cross Validation Accuracy = 96.46%\n",
      "c = 32, gamma = 8, accuracy = 96.46000000000001\n",
      "Cross Validation Accuracy = 96.72%\n",
      "c = 128, gamma = 3.0517578125e-05, accuracy = 96.72\n",
      "Cross Validation Accuracy = 96.72%\n",
      "c = 128, gamma = 0.0001220703125, accuracy = 96.72\n",
      "Cross Validation Accuracy = 96.62%\n",
      "c = 128, gamma = 0.00048828125, accuracy = 96.61999999999999\n",
      "Cross Validation Accuracy = 96.54%\n",
      "c = 128, gamma = 0.001953125, accuracy = 96.54\n",
      "Cross Validation Accuracy = 96.82%\n",
      "c = 128, gamma = 0.0078125, accuracy = 96.82\n",
      "Cross Validation Accuracy = 96.76%\n",
      "c = 128, gamma = 0.03125, accuracy = 96.76\n",
      "Cross Validation Accuracy = 96.62%\n",
      "c = 128, gamma = 0.125, accuracy = 96.61999999999999\n",
      "Cross Validation Accuracy = 96.48%\n",
      "c = 128, gamma = 0.5, accuracy = 96.48\n",
      "Cross Validation Accuracy = 96.54%\n",
      "c = 128, gamma = 2, accuracy = 96.54\n",
      "Cross Validation Accuracy = 96.28%\n",
      "c = 128, gamma = 8, accuracy = 96.28\n",
      "Cross Validation Accuracy = 96.44%\n",
      "c = 512, gamma = 3.0517578125e-05, accuracy = 96.44\n",
      "Cross Validation Accuracy = 96.68%\n",
      "c = 512, gamma = 0.0001220703125, accuracy = 96.67999999999999\n",
      "Cross Validation Accuracy = 96.48%\n",
      "c = 512, gamma = 0.00048828125, accuracy = 96.48\n",
      "Cross Validation Accuracy = 96.54%\n",
      "c = 512, gamma = 0.001953125, accuracy = 96.54\n",
      "Cross Validation Accuracy = 96.66%\n",
      "c = 512, gamma = 0.0078125, accuracy = 96.66\n",
      "Cross Validation Accuracy = 96.62%\n",
      "c = 512, gamma = 0.03125, accuracy = 96.61999999999999\n",
      "Cross Validation Accuracy = 96.44%\n",
      "c = 512, gamma = 0.125, accuracy = 96.44\n",
      "Cross Validation Accuracy = 96.94%\n",
      "c = 512, gamma = 0.5, accuracy = 96.94\n",
      "Cross Validation Accuracy = 96.62%\n",
      "c = 512, gamma = 2, accuracy = 96.61999999999999\n",
      "Cross Validation Accuracy = 96.54%\n",
      "c = 512, gamma = 8, accuracy = 96.54\n",
      "Cross Validation Accuracy = 96.54%\n",
      "c = 2048, gamma = 3.0517578125e-05, accuracy = 96.54\n",
      "Cross Validation Accuracy = 96.58%\n",
      "c = 2048, gamma = 0.0001220703125, accuracy = 96.58\n",
      "Cross Validation Accuracy = 96.58%\n",
      "c = 2048, gamma = 0.00048828125, accuracy = 96.58\n",
      "Cross Validation Accuracy = 96.46%\n",
      "c = 2048, gamma = 0.001953125, accuracy = 96.46000000000001\n",
      "Cross Validation Accuracy = 96.48%\n",
      "c = 2048, gamma = 0.0078125, accuracy = 96.48\n",
      "Cross Validation Accuracy = 96.62%\n",
      "c = 2048, gamma = 0.03125, accuracy = 96.61999999999999\n",
      "Cross Validation Accuracy = 96.66%\n",
      "c = 2048, gamma = 0.125, accuracy = 96.66\n",
      "Cross Validation Accuracy = 96.56%\n",
      "c = 2048, gamma = 0.5, accuracy = 96.56\n",
      "Cross Validation Accuracy = 96.78%\n",
      "c = 2048, gamma = 2, accuracy = 96.78\n",
      "Cross Validation Accuracy = 96.82%\n",
      "c = 2048, gamma = 8, accuracy = 96.82\n",
      "Cross Validation Accuracy = 96.58%\n",
      "c = 8192, gamma = 3.0517578125e-05, accuracy = 96.58\n",
      "Cross Validation Accuracy = 96.64%\n",
      "c = 8192, gamma = 0.0001220703125, accuracy = 96.64\n",
      "Cross Validation Accuracy = 96.4%\n",
      "c = 8192, gamma = 0.00048828125, accuracy = 96.39999999999999\n",
      "Cross Validation Accuracy = 96.44%\n",
      "c = 8192, gamma = 0.001953125, accuracy = 96.44\n",
      "Cross Validation Accuracy = 96.92%\n",
      "c = 8192, gamma = 0.0078125, accuracy = 96.92\n",
      "Cross Validation Accuracy = 96.54%\n",
      "c = 8192, gamma = 0.03125, accuracy = 96.54\n",
      "Cross Validation Accuracy = 96.38%\n",
      "c = 8192, gamma = 0.125, accuracy = 96.38\n",
      "Cross Validation Accuracy = 96.82%\n",
      "c = 8192, gamma = 0.5, accuracy = 96.82\n",
      "Cross Validation Accuracy = 96.74%\n",
      "c = 8192, gamma = 2, accuracy = 96.74000000000001\n",
      "Cross Validation Accuracy = 96.42%\n",
      "c = 8192, gamma = 8, accuracy = 96.41999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy = 96.5%\n",
      "c = 32768, gamma = 3.0517578125e-05, accuracy = 96.5\n",
      "Cross Validation Accuracy = 96.6%\n",
      "c = 32768, gamma = 0.0001220703125, accuracy = 96.6\n",
      "Cross Validation Accuracy = 96.44%\n",
      "c = 32768, gamma = 0.00048828125, accuracy = 96.44\n",
      "Cross Validation Accuracy = 96.34%\n",
      "c = 32768, gamma = 0.001953125, accuracy = 96.34\n",
      "Cross Validation Accuracy = 96.58%\n",
      "c = 32768, gamma = 0.0078125, accuracy = 96.58\n",
      "Cross Validation Accuracy = 96.56%\n",
      "c = 32768, gamma = 0.03125, accuracy = 96.56\n",
      "Cross Validation Accuracy = 96.62%\n",
      "c = 32768, gamma = 0.125, accuracy = 96.61999999999999\n",
      "Cross Validation Accuracy = 96.64%\n",
      "c = 32768, gamma = 0.5, accuracy = 96.64\n",
      "Cross Validation Accuracy = 96.62%\n",
      "c = 32768, gamma = 2, accuracy = 96.61999999999999\n",
      "Cross Validation Accuracy = 96.62%\n",
      "c = 32768, gamma = 8, accuracy = 96.61999999999999\n",
      "max_acc 97.39999999999999 max_c 0.03125 max_gamma 2\n",
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 96% (2400/2500) (classification)\n"
     ]
    }
   ],
   "source": [
    "# q2_2 linear kernel\n",
    "C = [2**-5, 2**-3, 2**-1, 2**1, 2**3, 2**5, 2**7, 2**9, 2**11, 2**13, 2**15]\n",
    "G = [2**-15, 2**-13, 2**-11, 2**-9, 2**-7, 2**-5, 2**-3, 2**-1, 2**1, 2**3]\n",
    "# C = [2, 8]\n",
    "# G = [0.5]\n",
    "max_acc = 0.0\n",
    "max_c = 0.0\n",
    "max_gamma = 0.0\n",
    "for c in C:\n",
    "    for gamma in G:\n",
    "        model = svm_train(y, x, '-t 0 -b 1 -s 0 -c {} -g {} -v 5'.format(c, gamma))\n",
    "        print('c = {}, gamma = {}, accuracy = {}'.format(c, gamma, model))\n",
    "        if model > max_acc:\n",
    "            max_acc = model\n",
    "            max_c = c\n",
    "            max_gamma = gamma\n",
    "print(\"max_acc\", max_acc, \"max_c\", max_c, \"max_gamma\", max_gamma)\n",
    "model = svm_train(y, x, '-t 0 -b 1 -s 0 -c {} -g {}'.format(max_c, max_gamma))\n",
    "p_label, p_acc, p_val = svm_predict(yt, xt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy = 97.16%\n",
      "c = 0.03125, accuracy = 97.16\n",
      "Cross Validation Accuracy = 97.08%\n",
      "c = 0.125, accuracy = 97.08\n",
      "Cross Validation Accuracy = 96.42%\n",
      "c = 0.5, accuracy = 96.41999999999999\n",
      "Cross Validation Accuracy = 96.64%\n",
      "c = 2, accuracy = 96.64\n",
      "Cross Validation Accuracy = 96.44%\n",
      "c = 8, accuracy = 96.44\n",
      "Cross Validation Accuracy = 96.84%\n",
      "c = 32, accuracy = 96.84\n",
      "Cross Validation Accuracy = 96.38%\n",
      "c = 128, accuracy = 96.38\n",
      "Cross Validation Accuracy = 96.46%\n",
      "c = 512, accuracy = 96.46000000000001\n",
      "Cross Validation Accuracy = 96.52%\n",
      "c = 2048, accuracy = 96.52\n",
      "Cross Validation Accuracy = 96.34%\n",
      "c = 8192, accuracy = 96.34\n",
      "Cross Validation Accuracy = 96.66%\n",
      "c = 32768, accuracy = 96.66\n",
      "max_acc 97.16 max_c 0.03125\n",
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 96% (2400/2500) (classification)\n"
     ]
    }
   ],
   "source": [
    "# q2_2 linear kernel\n",
    "C = [2**-5, 2**-3, 2**-1, 2**1, 2**3, 2**5, 2**7, 2**9, 2**11, 2**13, 2**15]\n",
    "#C = [2, 8]\n",
    "max_acc = 0.0\n",
    "max_c = 0.0\n",
    "for c in C:\n",
    "    model = svm_train(y, x, '-t 0 -b 1 -s 0 -c {} -v 5'.format(c))\n",
    "    print('c = {}, accuracy = {}'.format(c, model))\n",
    "    if model > max_acc:\n",
    "        max_acc = model\n",
    "        max_c = c\n",
    "print(\"max_acc\", max_acc, \"max_c\", max_c)\n",
    "model = svm_train(y, x, '-t 0 -b 1 -s 0 -c {}'.format(max_c))\n",
    "p_label, p_acc, p_val = svm_predict(yt, xt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy = 12.38%\n",
      "c = 0.03125, gamma = 3.0517578125e-05, accuracy = 12.379999999999999\n",
      "Cross Validation Accuracy = 38.44%\n",
      "c = 0.03125, gamma = 0.0001220703125, accuracy = 38.440000000000005\n",
      "Cross Validation Accuracy = 83.62%\n",
      "c = 0.03125, gamma = 0.00048828125, accuracy = 83.62\n",
      "Cross Validation Accuracy = 93.34%\n",
      "c = 0.03125, gamma = 0.001953125, accuracy = 93.34\n",
      "Cross Validation Accuracy = 95.1%\n",
      "c = 0.03125, gamma = 0.0078125, accuracy = 95.1\n",
      "Cross Validation Accuracy = 95.5%\n",
      "c = 0.03125, gamma = 0.03125, accuracy = 95.5\n",
      "Cross Validation Accuracy = 35%\n",
      "c = 0.03125, gamma = 0.125, accuracy = 35.0\n",
      "Cross Validation Accuracy = 19.2%\n",
      "c = 0.03125, gamma = 0.5, accuracy = 19.2\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 0.03125, gamma = 2, accuracy = 20.0\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 0.03125, gamma = 8, accuracy = 20.0\n",
      "Cross Validation Accuracy = 39.24%\n",
      "c = 0.125, gamma = 3.0517578125e-05, accuracy = 39.24\n",
      "Cross Validation Accuracy = 83.46%\n",
      "c = 0.125, gamma = 0.0001220703125, accuracy = 83.46000000000001\n",
      "Cross Validation Accuracy = 93.3%\n",
      "c = 0.125, gamma = 0.00048828125, accuracy = 93.30000000000001\n",
      "Cross Validation Accuracy = 95.4%\n",
      "c = 0.125, gamma = 0.001953125, accuracy = 95.39999999999999\n",
      "Cross Validation Accuracy = 96.56%\n",
      "c = 0.125, gamma = 0.0078125, accuracy = 96.56\n",
      "Cross Validation Accuracy = 97.28%\n",
      "c = 0.125, gamma = 0.03125, accuracy = 97.28\n",
      "Cross Validation Accuracy = 42.32%\n",
      "c = 0.125, gamma = 0.125, accuracy = 42.32\n",
      "Cross Validation Accuracy = 20.3%\n",
      "c = 0.125, gamma = 0.5, accuracy = 20.3\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 0.125, gamma = 2, accuracy = 20.0\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 0.125, gamma = 8, accuracy = 20.0\n",
      "Cross Validation Accuracy = 82.76%\n",
      "c = 0.5, gamma = 3.0517578125e-05, accuracy = 82.76\n",
      "Cross Validation Accuracy = 93.32%\n",
      "c = 0.5, gamma = 0.0001220703125, accuracy = 93.32000000000001\n",
      "Cross Validation Accuracy = 95.14%\n",
      "c = 0.5, gamma = 0.00048828125, accuracy = 95.14\n",
      "Cross Validation Accuracy = 96.46%\n",
      "c = 0.5, gamma = 0.001953125, accuracy = 96.46000000000001\n",
      "Cross Validation Accuracy = 97.4%\n",
      "c = 0.5, gamma = 0.0078125, accuracy = 97.39999999999999\n",
      "Cross Validation Accuracy = 98.26%\n",
      "c = 0.5, gamma = 0.03125, accuracy = 98.26\n",
      "Cross Validation Accuracy = 79.04%\n",
      "c = 0.5, gamma = 0.125, accuracy = 79.03999999999999\n",
      "Cross Validation Accuracy = 31.92%\n",
      "c = 0.5, gamma = 0.5, accuracy = 31.919999999999998\n",
      "Cross Validation Accuracy = 19.76%\n",
      "c = 0.5, gamma = 2, accuracy = 19.759999999999998\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 0.5, gamma = 8, accuracy = 20.0\n",
      "Cross Validation Accuracy = 93.42%\n",
      "c = 2, gamma = 3.0517578125e-05, accuracy = 93.42\n",
      "Cross Validation Accuracy = 95.34%\n",
      "c = 2, gamma = 0.0001220703125, accuracy = 95.34\n",
      "Cross Validation Accuracy = 96.44%\n",
      "c = 2, gamma = 0.00048828125, accuracy = 96.44\n",
      "Cross Validation Accuracy = 97.12%\n",
      "c = 2, gamma = 0.001953125, accuracy = 97.11999999999999\n",
      "Cross Validation Accuracy = 98.12%\n",
      "c = 2, gamma = 0.0078125, accuracy = 98.11999999999999\n",
      "Cross Validation Accuracy = 98.68%\n",
      "c = 2, gamma = 0.03125, accuracy = 98.68\n",
      "Cross Validation Accuracy = 96.32%\n",
      "c = 2, gamma = 0.125, accuracy = 96.32\n",
      "Cross Validation Accuracy = 33.64%\n",
      "c = 2, gamma = 0.5, accuracy = 33.64\n",
      "Cross Validation Accuracy = 24.66%\n",
      "c = 2, gamma = 2, accuracy = 24.66\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 2, gamma = 8, accuracy = 20.0\n",
      "Cross Validation Accuracy = 95.26%\n",
      "c = 8, gamma = 3.0517578125e-05, accuracy = 95.26\n",
      "Cross Validation Accuracy = 96.3%\n",
      "c = 8, gamma = 0.0001220703125, accuracy = 96.3\n",
      "Cross Validation Accuracy = 97%\n",
      "c = 8, gamma = 0.00048828125, accuracy = 97.0\n",
      "Cross Validation Accuracy = 97.48%\n",
      "c = 8, gamma = 0.001953125, accuracy = 97.48\n",
      "Cross Validation Accuracy = 98.12%\n",
      "c = 8, gamma = 0.0078125, accuracy = 98.11999999999999\n",
      "Cross Validation Accuracy = 98.62%\n",
      "c = 8, gamma = 0.03125, accuracy = 98.61999999999999\n",
      "Cross Validation Accuracy = 96.2%\n",
      "c = 8, gamma = 0.125, accuracy = 96.2\n",
      "Cross Validation Accuracy = 37.36%\n",
      "c = 8, gamma = 0.5, accuracy = 37.36\n",
      "Cross Validation Accuracy = 24.62%\n",
      "c = 8, gamma = 2, accuracy = 24.62\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 8, gamma = 8, accuracy = 20.0\n",
      "Cross Validation Accuracy = 96.26%\n",
      "c = 32, gamma = 3.0517578125e-05, accuracy = 96.26\n",
      "Cross Validation Accuracy = 96.82%\n",
      "c = 32, gamma = 0.0001220703125, accuracy = 96.82\n",
      "Cross Validation Accuracy = 97.2%\n",
      "c = 32, gamma = 0.00048828125, accuracy = 97.2\n",
      "Cross Validation Accuracy = 97.5%\n",
      "c = 32, gamma = 0.001953125, accuracy = 97.5\n",
      "Cross Validation Accuracy = 98.16%\n",
      "c = 32, gamma = 0.0078125, accuracy = 98.16\n",
      "Cross Validation Accuracy = 98.56%\n",
      "c = 32, gamma = 0.03125, accuracy = 98.56\n",
      "Cross Validation Accuracy = 96.14%\n",
      "c = 32, gamma = 0.125, accuracy = 96.14\n",
      "Cross Validation Accuracy = 32.12%\n",
      "c = 32, gamma = 0.5, accuracy = 32.12\n",
      "Cross Validation Accuracy = 24.38%\n",
      "c = 32, gamma = 2, accuracy = 24.38\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 32, gamma = 8, accuracy = 20.0\n",
      "Cross Validation Accuracy = 96.84%\n",
      "c = 128, gamma = 3.0517578125e-05, accuracy = 96.84\n",
      "Cross Validation Accuracy = 97.02%\n",
      "c = 128, gamma = 0.0001220703125, accuracy = 97.02\n",
      "Cross Validation Accuracy = 97.38%\n",
      "c = 128, gamma = 0.00048828125, accuracy = 97.38\n",
      "Cross Validation Accuracy = 97.58%\n",
      "c = 128, gamma = 0.001953125, accuracy = 97.58\n",
      "Cross Validation Accuracy = 98.14%\n",
      "c = 128, gamma = 0.0078125, accuracy = 98.14\n",
      "Cross Validation Accuracy = 98.66%\n",
      "c = 128, gamma = 0.03125, accuracy = 98.66\n",
      "Cross Validation Accuracy = 96.28%\n",
      "c = 128, gamma = 0.125, accuracy = 96.28\n",
      "Cross Validation Accuracy = 35.24%\n",
      "c = 128, gamma = 0.5, accuracy = 35.24\n",
      "Cross Validation Accuracy = 24.5%\n",
      "c = 128, gamma = 2, accuracy = 24.5\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 128, gamma = 8, accuracy = 20.0\n",
      "Cross Validation Accuracy = 97.32%\n",
      "c = 512, gamma = 3.0517578125e-05, accuracy = 97.32\n",
      "Cross Validation Accuracy = 97.1%\n",
      "c = 512, gamma = 0.0001220703125, accuracy = 97.1\n",
      "Cross Validation Accuracy = 96.94%\n",
      "c = 512, gamma = 0.00048828125, accuracy = 96.94\n",
      "Cross Validation Accuracy = 97.46%\n",
      "c = 512, gamma = 0.001953125, accuracy = 97.46000000000001\n",
      "Cross Validation Accuracy = 98.28%\n",
      "c = 512, gamma = 0.0078125, accuracy = 98.28\n",
      "Cross Validation Accuracy = 98.58%\n",
      "c = 512, gamma = 0.03125, accuracy = 98.58\n",
      "Cross Validation Accuracy = 96.12%\n",
      "c = 512, gamma = 0.125, accuracy = 96.12\n",
      "Cross Validation Accuracy = 33.42%\n",
      "c = 512, gamma = 0.5, accuracy = 33.42\n",
      "Cross Validation Accuracy = 24.78%\n",
      "c = 512, gamma = 2, accuracy = 24.779999999999998\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 512, gamma = 8, accuracy = 20.0\n",
      "Cross Validation Accuracy = 97.02%\n",
      "c = 2048, gamma = 3.0517578125e-05, accuracy = 97.02\n",
      "Cross Validation Accuracy = 96.66%\n",
      "c = 2048, gamma = 0.0001220703125, accuracy = 96.66\n",
      "Cross Validation Accuracy = 97.2%\n",
      "c = 2048, gamma = 0.00048828125, accuracy = 97.2\n",
      "Cross Validation Accuracy = 97.6%\n",
      "c = 2048, gamma = 0.001953125, accuracy = 97.6\n",
      "Cross Validation Accuracy = 98.16%\n",
      "c = 2048, gamma = 0.0078125, accuracy = 98.16\n",
      "Cross Validation Accuracy = 98.72%\n",
      "c = 2048, gamma = 0.03125, accuracy = 98.72\n",
      "Cross Validation Accuracy = 95.96%\n",
      "c = 2048, gamma = 0.125, accuracy = 95.96000000000001\n",
      "Cross Validation Accuracy = 33.64%\n",
      "c = 2048, gamma = 0.5, accuracy = 33.64\n",
      "Cross Validation Accuracy = 24.72%\n",
      "c = 2048, gamma = 2, accuracy = 24.72\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 2048, gamma = 8, accuracy = 20.0\n",
      "Cross Validation Accuracy = 96.72%\n",
      "c = 8192, gamma = 3.0517578125e-05, accuracy = 96.72\n",
      "Cross Validation Accuracy = 96.82%\n",
      "c = 8192, gamma = 0.0001220703125, accuracy = 96.82\n",
      "Cross Validation Accuracy = 97.16%\n",
      "c = 8192, gamma = 0.00048828125, accuracy = 97.16\n",
      "Cross Validation Accuracy = 97.84%\n",
      "c = 8192, gamma = 0.001953125, accuracy = 97.84\n",
      "Cross Validation Accuracy = 98.16%\n",
      "c = 8192, gamma = 0.0078125, accuracy = 98.16\n",
      "Cross Validation Accuracy = 98.58%\n",
      "c = 8192, gamma = 0.03125, accuracy = 98.58\n",
      "Cross Validation Accuracy = 96.14%\n",
      "c = 8192, gamma = 0.125, accuracy = 96.14\n",
      "Cross Validation Accuracy = 33.42%\n",
      "c = 8192, gamma = 0.5, accuracy = 33.42\n",
      "Cross Validation Accuracy = 24.62%\n",
      "c = 8192, gamma = 2, accuracy = 24.62\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 8192, gamma = 8, accuracy = 20.0\n",
      "Cross Validation Accuracy = 96.66%\n",
      "c = 32768, gamma = 3.0517578125e-05, accuracy = 96.66\n",
      "Cross Validation Accuracy = 96.64%\n",
      "c = 32768, gamma = 0.0001220703125, accuracy = 96.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy = 97.22%\n",
      "c = 32768, gamma = 0.00048828125, accuracy = 97.22\n",
      "Cross Validation Accuracy = 97.66%\n",
      "c = 32768, gamma = 0.001953125, accuracy = 97.66\n",
      "Cross Validation Accuracy = 98.2%\n",
      "c = 32768, gamma = 0.0078125, accuracy = 98.2\n",
      "Cross Validation Accuracy = 98.48%\n",
      "c = 32768, gamma = 0.03125, accuracy = 98.48\n",
      "Cross Validation Accuracy = 96.16%\n",
      "c = 32768, gamma = 0.125, accuracy = 96.16\n",
      "Cross Validation Accuracy = 35.04%\n",
      "c = 32768, gamma = 0.5, accuracy = 35.04\n",
      "Cross Validation Accuracy = 24.7%\n",
      "c = 32768, gamma = 2, accuracy = 24.7\n",
      "Cross Validation Accuracy = 20%\n",
      "c = 32768, gamma = 8, accuracy = 20.0\n",
      "max_acc 98.72 max_c 2048 max_gamma 0.03125\n",
      "Model supports probability estimates, but disabled in predicton.\n",
      "Accuracy = 98.52% (2463/2500) (classification)\n"
     ]
    }
   ],
   "source": [
    "# q2_2 RBF kernel\n",
    "C = [2**-5, 2**-3, 2**-1, 2**1, 2**3, 2**5, 2**7, 2**9, 2**11, 2**13, 2**15]\n",
    "G = [2**-15, 2**-13, 2**-11, 2**-9, 2**-7, 2**-5, 2**-3, 2**-1, 2**1, 2**3]\n",
    "# C = [2, 8]\n",
    "# G = [0.5]\n",
    "max_acc = 0.0\n",
    "max_c = 0.0\n",
    "max_gamma = 0.0\n",
    "for c in C:\n",
    "    for gamma in G:\n",
    "        model = svm_train(y, x, '-t 2 -b 1 -s 0 -c {} -g {} -v 5'.format(c, gamma))\n",
    "        print('c = {}, gamma = {}, accuracy = {}'.format(c, gamma, model))\n",
    "        if model > max_acc:\n",
    "            max_acc = model\n",
    "            max_c = c\n",
    "            max_gamma = gamma\n",
    "print(\"max_acc\", max_acc, \"max_c\", max_c, \"max_gamma\", max_gamma)\n",
    "model = svm_train(y, x, '-t 2 -b 1 -s 0 -c {} -g {}'.format(max_c, max_gamma))\n",
    "p_label, p_acc, p_val = svm_predict(yt, xt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # q2_2\n",
    "# C = [2**-5, 2**-3, 2**-1, 2**1, 2**3, 2**5, 2**7, 2**9, 2**11, 2**13, 2**15]\n",
    "# G = [2**-15, 2**-13, 2**-11, 2**-9, 2**-7, 2**-5, 2**-3, 2**-1, 2**1, 2**3]\n",
    "# # C = [2, 8]\n",
    "# # G = [0.5]\n",
    "# max_acc = 0.0\n",
    "# max_c = 0.0\n",
    "# max_gamma = 0.0\n",
    "# for c in C:\n",
    "#     for gamma in G:\n",
    "#         model = svm_train(y, x, '-t 2 -b 1 -s 0 -c {} -g {} -v 5'.format(c, gamma))\n",
    "#         print('c = {}, gamma = {}, accuracy = {}'.format(c, gamma, model))\n",
    "#         if model > max_acc:\n",
    "#             max_acc = model\n",
    "#             max_c = c\n",
    "#             max_gamma = gamma\n",
    "# print(\"max_acc\", max_acc, \"max_c\", max_c, \"max_gamma\", max_gamma)\n",
    "# model = svm_train(y, x, '-t 2 -b 1 -s 0 -c {} -g {}'.format(max_c, max_gamma))\n",
    "# p_label, p_acc, p_val = svm_predict(yt, xt, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
